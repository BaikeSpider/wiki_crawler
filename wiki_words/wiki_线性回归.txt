线性回归






本条目存在以下问题，请协助改善本条目或在讨论页针对议题发表看法。






本条目包含过多行话或专业术语，可能需要简化或提出进一步解释。（2013年10月5日） 
请在讨论页中发表对于本议题的看法，并移除或解释本条目中的行话。 







本条目翻译品质不佳。 
翻译者可能不熟悉中文或原文语言，也可能使用了机器翻译，请协助翻译本条目或重新编写。明显拙劣的机器翻译请改挂{{Delete|G13}}提交删除。 














统计学系列条目


回归分析





模型




线性回归
简单回归（英语：Simple linear regression）
普通最小二乘法（英语：Ordinary least squares）
多项式回归（英语：Polynomial regression）
一般线性模型






广义线性模式
离散选择（英语：Discrete choice）
逻辑回归
多项罗吉特（英语：Multinomial logit）
混合罗吉特
波比（英语：Probit model）
多项式波比（英语：Multinomial probit）
排序性模型（英语：Ordered logit）
有序波比（英语：Ordered probit）
泊松回归






等级线性模型
固定效应（英语：Fixed effects model）
随机效应（英语：Random effects model）
混合模型（英语：Mixed model）






非线性回归（英语：Nonlinear regression）
非参数（英语：Nonparametric regression）
半参数（英语：Semiparametric regression）
稳健（英语：Robust regression）
分位数回归
保序回归
主成分（英语：Principal component regression）
最小角
局部（英语：Local regression）
分段（英语：Segmented regression）






含误差变量（英语：Errors-in-variables models）




估计




最小二乘法
普通最小二乘法（英语：Ordinary least squares）
线性
偏最小二乘回归
总体（英语：Total least squares）
广义（英语：Generalized least squares）
加权
非线性（英语：Non-linear least squares）
非负（英语：Non-negative least squares）
重复再加权（英语：Iteratively reweighted least squares）
岭回归（英语：Tikhonov regularization）
LASSO






最小绝对值导数法（英语：Least absolute deviations）
贝叶斯（英语：Bayesian linear regression）
贝叶斯多元（英语：Bayesian multivariate linear regression）




背景




回归模型检验（英语：Regression model validation）
平均响应和预测响应（英语：Mean and predicted response）
误差和残差
拟合优度（英语：Goodness of fit）
学生化残差（英语：Studentized residual）
高斯－马尔可夫定理






 概率与统计主题







查
论
编





在统计学中，线性回归（Linear regression）是利用称为线性回归方程的最小二乘函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。（这反过来又应当由多个相关的因变量预测的多元线性回归区别[来源请求]，而不是一个单一的标量变量。）
在线性回归中，数据使用线性预测函数来建模，并且未知的模型参数也是通过数据来估计。这些模型被叫做线性模型。最常用的线性回归建模是给定X值的y的条件均值是X的仿射函数。不太一般的情况，线性回归模型可以是一个中位数或一些其他的给定X的条件下y的条件分布的分位数作为X的线性函数表示。像所有形式的回归分析一样，线性回归也把焦点放在给定X值的y的条件概率分布，而不是X和y的联合概率分布（多元分析领域）。
线性回归是回归分析中第一种经过严格研究并在实际应用中广泛使用的类型。这是因为线性依赖于其未知参数的模型比非线性依赖于其未知参数的模型更容易拟合，而且产生的估计的统计特性也更容易确定。
线性回归有很多实际用途。分为以下两大类：

如果目标是预测或者映射，线性回归可以用来对观测数据集的和X的值拟合出一个预测模型。当完成这样一个模型以后，对于一个新增的X值，在没有给定与它相配对的y的情况下，可以用这个拟合过的模型预测出一个y值。
给定一个变量y和一些变量X1,...,Xp，这些变量有可能与y相关，线性回归分析可以用来量化y与Xj之间相关性的强度，评估出与y不相关的Xj，并识别出哪些Xj的子集包含了关于y的冗余信息。

线性回归模型经常用最小二乘逼近来拟合，但他们也可能用别的方法来拟合，比如用最小化“拟合缺陷”在一些其他规范里（比如最小绝对误差回归），或者在桥回归中最小化最小二乘损失函数的惩罚。相反，最小二乘逼近可以用来拟合那些非线性的模型。因此，尽管“最小二乘法”和“线性模型”是紧密相连的，但他们是不能划等号的。



目录


1 简介

1.1 理论模型
1.2 数据和估计
1.3 古典假设


2 最小二乘法分析

2.1 最小二乘法估计
2.2 回归推论

2.2.1 单变量线性回归


2.3 方差分析


3 其他方法

3.1 广义最小二乘法
3.2 总体最小二乘法
3.3 广义线性模式
3.4 稳健回归


4 线性回归的应用

4.1 趋势线
4.2 流行病学
4.3 金融
4.4 经济学


5 参考文献

5.1 引用
5.2 来源


6 延伸阅读
7 参见



简介[编辑]




带有一个自变量的线性回归


理论模型[编辑]
给一个随机样本



(

Y

i


,

X

i
1


,
…
,

X

i
p


)
,

i
=
1
,
…
,
n


{\displaystyle (Y_{i},X_{i1},\ldots ,X_{ip}),\,i=1,\ldots ,n}

，一个线性回归模型假设回归子




Y

i




{\displaystyle Y_{i}}

和回归量




X

i
1


,
…
,

X

i
p




{\displaystyle X_{i1},\ldots ,X_{ip}}

之间的关系是除了X的影响以外，还有其他的变数存在。我们加入一个误差项




ε

i




{\displaystyle \varepsilon _{i}}

（也是一个随机变量）来捕获除了




X

i
1


,
…
,

X

i
p




{\displaystyle X_{i1},\ldots ,X_{ip}}

之外任何对




Y

i




{\displaystyle Y_{i}}

的影响。所以一个多变量线性回归模型表示为以下的形式：






Y

i


=

β

0


+

β

1



X

i
1


+

β

2



X

i
2


+
…
+

β

p



X

i
p


+

ε

i


,

i
=
1
,
…
,
n


{\displaystyle Y_{i}=\beta _{0}+\beta _{1}X_{i1}+\beta _{2}X_{i2}+\ldots +\beta _{p}X_{ip}+\varepsilon _{i},\qquad i=1,\ldots ,n}



其他的模型可能被认定成非线性模型。一个线性回归模型不需要是自变量的线性函数。线性在这里表示




Y

i




{\displaystyle Y_{i}}

的条件均值在参数



β


{\displaystyle \beta }

里是线性的。例如：模型




Y

i


=

β

1



X

i


+

β

2



X

i


2


+

ε

i




{\displaystyle Y_{i}=\beta _{1}X_{i}+\beta _{2}X_{i}^{2}+\varepsilon _{i}}

在




β

1




{\displaystyle \beta _{1}}

和




β

2




{\displaystyle \beta _{2}}

里是线性的，但在




X

i


2




{\displaystyle X_{i}^{2}}

里是非线性的，它是




X

i




{\displaystyle X_{i}}

的非线性函数。
数据和估计[编辑]
区分随机变量和这些变量的观测值是很重要的。通常来说，观测值或数据（以小写字母表记）包括了n个值 



(

y

i


,

x

i
1


,
…
,

x

i
p


)
,

i
=
1
,
…
,
n


{\displaystyle (y_{i},x_{i1},\ldots ,x_{ip}),\,i=1,\ldots ,n}

.
我们有



p
+
1


{\displaystyle p+1}

个参数




β

0


,
…
,

β

p




{\displaystyle \beta _{0},\ldots ,\beta _{p}}

需要决定，为了估计这些参数，使用矩阵表记是很有用的。





Y
=
X
β
+
ε



{\displaystyle Y=X\beta +\varepsilon \,}



其中Y是一个包括了观测值




Y

1


,
…
,

Y

n




{\displaystyle Y_{1},\ldots ,Y_{n}}

的列向量，



ε


{\displaystyle \varepsilon }

包括了未观测的随机成分




ε

1


,
…
,

ε

n




{\displaystyle \varepsilon _{1},\ldots ,\varepsilon _{n}}

以及回归量的观测值矩阵



X


{\displaystyle X}

：





X
=


(



1



x

11




⋯



x

1
p






1



x

21




⋯



x

2
p






⋮


⋮


⋱


⋮




1



x

n
1




⋯



x

n
p





)




{\displaystyle X={\begin{pmatrix}1&x_{11}&\cdots &x_{1p}\\1&x_{21}&\cdots &x_{2p}\\\vdots &\vdots &\ddots &\vdots \\1&x_{n1}&\cdots &x_{np}\end{pmatrix}}}



X通常包括一个常数项。
如果X列之间存在线性相关，那么参数向量



β


{\displaystyle \beta }

就不能以最小二乘法估计除非



β


{\displaystyle \beta }

被限制，比如要求它的一些元素之和为0。
古典假设[编辑]

样本是在母体之中随机抽取出来的。
因变量Y在实直线上是连续的，
残差项是独立且相同分布的(iid)，也就是说，残差是独立随机的，且服从高斯分布。

这些假设意味着残差项不依赖自变量的值，所以




ε

i




{\displaystyle \varepsilon _{i}}

和自变量X（预测变量）之间是相互独立的。
在这些假设下，建立一个显示线性回归作为条件预期模型的简单线性回归，可以表示为：







E


(

Y

i


∣

X

i


=

x

i


)
=
α
+
β

x

i





{\displaystyle {\mbox{E}}(Y_{i}\mid X_{i}=x_{i})=\alpha +\beta x_{i}\,}



最小二乘法分析[编辑]
主条目：最小二乘法
最小二乘法估计[编辑]
回归分析的最初目的是估计模型的参数以便达到对数据的最佳拟合。在决定一个最佳拟合的不同标准之中，最小二乘法是非常优越的。这种估计可以表示为：








β
^



=
(

X

T


X

)

−
1



X

T


y



{\displaystyle {\hat {\beta }}=(X^{T}X)^{-1}X^{T}y\,}



回归推论[编辑]
对于每一个



i
=
1
,
…
,
n


{\displaystyle i=1,\ldots ,n}

，我们用




σ

2




{\displaystyle \sigma ^{2}}

代表误差项



ε


{\displaystyle \varepsilon }

的方差。一个无偏误的估计是：









σ
^




2


=


S

n
−
p



,


{\displaystyle {\hat {\sigma }}^{2}={\frac {S}{n-p}},}



其中



S
:=

∑

i
=
1


n






ε
^




i


2




{\displaystyle S:=\sum _{i=1}^{n}{\hat {\varepsilon }}_{i}^{2}}

是误差平方和（残差平方和）。估计值和实际值之间的关系是：









σ
^




2


⋅



n
−
p


σ

2




∼

χ

n
−
p


2




{\displaystyle {\hat {\sigma }}^{2}\cdot {\frac {n-p}{\sigma ^{2}}}\sim \chi _{n-p}^{2}}



其中




χ

n
−
p


2




{\displaystyle \chi _{n-p}^{2}}

服从卡方分布，自由度是



n
−
p


{\displaystyle n-p}


对普通方程的解可以冩为：








β
^



=
(


X

T


X

)

−
1



X

T


y

.


{\displaystyle {\hat {\boldsymbol {\beta }}}=(\mathbf {X^{T}X)^{-1}X^{T}y} .}



这表示估计项是因变量的线性组合。进一步地说，如果所观察的误差服从正态分布。参数的估计值将服从联合正态分布。在当前的假设之下，估计的参数向量是精确分布的。








β
^



∼
N
(
β
,

σ

2


(

X

T


X

)

−
1


)


{\displaystyle {\hat {\beta }}\sim N(\beta ,\sigma ^{2}(X^{T}X)^{-1})}



其中



N
(
⋅
)


{\displaystyle N(\cdot )}

表示多变量正态分布。
参数估计值的标准差是：









σ
^




j


=




S

n
−
p





[


(

X

T


X
)


−
1


]


j
j




.


{\displaystyle {\hat {\sigma }}_{j}={\sqrt {{\frac {S}{n-p}}\left[\mathbf {(X^{T}X)} ^{-1}\right]_{jj}}}.}



参数




β

j




{\displaystyle \beta _{j}}

的



100
(
1
−
α
)
%


{\displaystyle 100(1-\alpha )\%}

置信区间可以用以下式子来计算：









β
^




j


±

t



α
2


,
n
−
p






σ
^




j


.


{\displaystyle {\hat {\beta }}_{j}\pm t_{{\frac {\alpha }{2}},n-p}{\hat {\sigma }}_{j}.}



误差项可以表示为：









r
^



=
y
−
X



β
^



=
y
−
X
(

X

T


X

)

−
1



X

T


y

.



{\displaystyle \mathbf {{\hat {r}}=y-X{\hat {\boldsymbol {\beta }}}=y-X(X^{T}X)^{-1}X^{T}y} .\,}



单变量线性回归[编辑]
单变量线性回归，又称简单线性回归（simple linear regression, SLR），是最简单但用途很广的回归模型。其回归式为：





Y
=
α
+
β
X
+
ε


{\displaystyle Y=\alpha +\beta X+\varepsilon }



为了从一组样本



(

y

i


,

x

i


)


{\displaystyle (y_{i},x_{i})}

（其中



i
=
1
,
 
2
,
…
,
n


{\displaystyle i=1,\ 2,\ldots ,n}

）之中估计最合适（误差最小）的



α


{\displaystyle \alpha }

和



β


{\displaystyle \beta }

，通常采用最小二乘法，其计算目标为最小化残差平方和：






∑

i
=
1


n



ε

i


2


=

∑

i
=
1


n


(

y

i


−
α
−
β

x

i



)

2




{\displaystyle \sum _{i=1}^{n}\varepsilon _{i}^{2}=\sum _{i=1}^{n}(y_{i}-\alpha -\beta x_{i})^{2}}



使用微分法求极值：将上式分别对



α


{\displaystyle \alpha }

和



β


{\displaystyle \beta }

做一阶偏微分，并令其等于0：






{




n
 
α
+

∑

i
=
1


n



x

i


 
β
=

∑

i
=
1


n



y

i







∑

i
=
1


n



x

i


 
α
+

∑

i
=
1


n



x

i


2


 
β
=

∑

i
=
1


n



x

i



y

i










{\displaystyle \left\{{\begin{array}{lcl}n\ \alpha +\sum \limits _{i=1}^{n}x_{i}\ \beta =\sum \limits _{i=1}^{n}y_{i}\\\sum \limits _{i=1}^{n}x_{i}\ \alpha +\sum \limits _{i=1}^{n}x_{i}^{2}\ \beta =\sum \limits _{i=1}^{n}x_{i}y_{i}\end{array}}\right.}



此二元一次线性方程组可用克莱姆法则求解，得解






α
^



,
 



β
^





{\displaystyle {\hat {\alpha }},\ {\hat {\beta }}}

：








β
^



=



n

∑

i
=
1


n



x

i



y

i


−

∑

i
=
1


n



x

i



∑

i
=
1


n



y

i




n

∑

i
=
1


n



x

i


2


−


(


∑

i
=
1


n



x

i



)


2





=




∑

i
=
1


n


(

x

i


−



x
¯



)
(

y

i


−



y
¯



)



∑

i
=
1


n


(

x

i


−



x
¯




)

2








{\displaystyle {\hat {\beta }}={\frac {n\sum \limits _{i=1}^{n}x_{i}y_{i}-\sum \limits _{i=1}^{n}x_{i}\sum \limits _{i=1}^{n}y_{i}}{n\sum \limits _{i=1}^{n}x_{i}^{2}-\left(\sum \limits _{i=1}^{n}x_{i}\right)^{2}}}={\frac {\sum \limits _{i=1}^{n}(x_{i}-{\bar {x}})(y_{i}-{\bar {y}})}{\sum \limits _{i=1}^{n}(x_{i}-{\bar {x}})^{2}}}\,}











α
^



=




∑

i
=
1


n



x

i


2



∑

i
=
1


n



y

i


−

∑

i
=
1


n



x

i



∑

i
=
1


n



x

i



y

i




n

∑

i
=
1


n



x

i


2


−


(


∑

i
=
1


n



x

i



)


2





=



y
¯



−



x
¯






β
^





{\displaystyle {\hat {\alpha }}={\frac {\sum \limits _{i=1}^{n}x_{i}^{2}\sum \limits _{i=1}^{n}y_{i}-\sum \limits _{i=1}^{n}x_{i}\sum \limits _{i=1}^{n}x_{i}y_{i}}{n\sum \limits _{i=1}^{n}x_{i}^{2}-\left(\sum \limits _{i=1}^{n}x_{i}\right)^{2}}}={\bar {y}}-{\bar {x}}{\hat {\beta }}}








S
=

∑

i
=
1


n


(

y

i


−




y
^




i



)

2


=

∑

i
=
1


n



y

i


2


−



n
(

∑

i
=
1


n



x

i



y

i



)

2


+
(

∑

i
=
1


n



y

i



)

2



∑

i
=
1


n



x

i


2


−
2

∑

i
=
1


n



x

i



∑

i
=
1


n



y

i



∑

i
=
1


n



x

i



y

i




n

∑

i
=
1


n



x

i


2


−


(


∑

i
=
1


n



x

i



)


2







{\displaystyle S=\sum \limits _{i=1}^{n}(y_{i}-{\hat {y}}_{i})^{2}=\sum \limits _{i=1}^{n}y_{i}^{2}-{\frac {n(\sum \limits _{i=1}^{n}x_{i}y_{i})^{2}+(\sum \limits _{i=1}^{n}y_{i})^{2}\sum \limits _{i=1}^{n}x_{i}^{2}-2\sum \limits _{i=1}^{n}x_{i}\sum \limits _{i=1}^{n}y_{i}\sum \limits _{i=1}^{n}x_{i}y_{i}}{n\sum \limits _{i=1}^{n}x_{i}^{2}-\left(\sum \limits _{i=1}^{n}x_{i}\right)^{2}}}}












σ
^




2


=


S

n
−
2



.


{\displaystyle {\hat {\sigma }}^{2}={\frac {S}{n-2}}.}



协方差矩阵是：







1

n

∑

i
=
1


n



x

i


2


−


(


∑

i
=
1


n



x

i



)


2







(



∑

x

i


2




−
∑

x

i






−
∑

x

i




n



)




{\displaystyle {\frac {1}{n\sum _{i=1}^{n}x_{i}^{2}-\left(\sum _{i=1}^{n}x_{i}\right)^{2}}}{\begin{pmatrix}\sum x_{i}^{2}&-\sum x_{i}\\-\sum x_{i}&n\end{pmatrix}}}



平均响应置信区间为：






y

d


=
(
α
+



β
^




x

d


)
±

t



α
2


,
n
−
2





σ
^







1
n


+



(

x

d


−



x
¯




)

2




∑
(

x

i


−



x
¯




)

2









{\displaystyle y_{d}=(\alpha +{\hat {\beta }}x_{d})\pm t_{{\frac {\alpha }{2}},n-2}{\hat {\sigma }}{\sqrt {{\frac {1}{n}}+{\frac {(x_{d}-{\bar {x}})^{2}}{\sum (x_{i}-{\bar {x}})^{2}}}}}}



预报响应置信区间为：






y

d


=
(
α
+



β
^




x

d


)
±

t



α
2


,
n
−
2





σ
^





1
+


1
n


+



(

x

d


−



x
¯




)

2




∑
(

x

i


−



x
¯




)

2









{\displaystyle y_{d}=(\alpha +{\hat {\beta }}x_{d})\pm t_{{\frac {\alpha }{2}},n-2}{\hat {\sigma }}{\sqrt {1+{\frac {1}{n}}+{\frac {(x_{d}-{\bar {x}})^{2}}{\sum (x_{i}-{\bar {x}})^{2}}}}}}



方差分析[编辑]
在方差分析（ANOVA）中，总平方和分解为两个或更多部分。
总平方和SST (sum of squares for total) 是：






SST

=

∑

i
=
1


n


(

y

i


−



y
¯




)

2




{\displaystyle {\text{SST}}=\sum _{i=1}^{n}(y_{i}-{\bar {y}})^{2}}

　，其中：　






y
¯



=


1
n



∑

i



y

i




{\displaystyle {\bar {y}}={\frac {1}{n}}\sum _{i}y_{i}}



同等地：






SST

=

∑

i
=
1


n



y

i


2


−


1
n




(


∑

i



y

i



)


2




{\displaystyle {\text{SST}}=\sum _{i=1}^{n}y_{i}^{2}-{\frac {1}{n}}\left(\sum _{i}y_{i}\right)^{2}}



回归平方和SSReg (sum of squares for regression。也可写做模型平方和，SSM，sum of squares for model) 是：






SSReg

=
∑


(





y
^




i


−



y
¯




)


2


=




β
^




T




X


T



y

−


1
n



(


y

T


u

u

T


y

)

,


{\displaystyle {\text{SSReg}}=\sum \left({\hat {y}}_{i}-{\bar {y}}\right)^{2}={\hat {\boldsymbol {\beta }}}^{T}\mathbf {X} ^{T}\mathbf {y} -{\frac {1}{n}}\left(\mathbf {y^{T}uu^{T}y} \right),}



残差平方和SSE (sum of squares for error) 是：






SSE

=

∑

i





(


y

i


−




y
^




i



)


2



=


y

T


y
−




β
^




T



X

T


y

.


{\displaystyle {\text{SSE}}=\sum _{i}{\left({y_{i}-{\hat {y}}_{i}}\right)^{2}}=\mathbf {y^{T}y-{\hat {\boldsymbol {\beta }}}^{T}X^{T}y} .}



总平方和SST又可写做SSReg和SSE的和：






SST

=

∑

i




(


y

i


−



y
¯




)


2


=


y

T


y

−


1
n



(


y

T


u

u

T


y

)

=

SSReg

+

SSE

.


{\displaystyle {\text{SST}}=\sum _{i}\left(y_{i}-{\bar {y}}\right)^{2}=\mathbf {y^{T}y} -{\frac {1}{n}}\left(\mathbf {y^{T}uu^{T}y} \right)={\text{SSReg}}+{\text{SSE}}.}



回归系数R2是：






R

2


=


SSReg
SST


=
1
−


SSE
SST


.


{\displaystyle R^{2}={\frac {\text{SSReg}}{\text{SST}}}=1-{\frac {\text{SSE}}{\text{SST}}}.}



其他方法[编辑]
广义最小二乘法[编辑]
广义最小二乘法可以用在当观测误差具有异方差或者自相关的情况下。
总体最小二乘法[编辑]
总体最小二乘法用于当自变量有误时。
广义线性模式[编辑]
广义线性模式应用在当误差分布函数不是正态分布时。比如指数分布，伽玛分布，逆高斯分布，泊松分布，二项式分布等。
稳健回归[编辑]
将平均绝对误差最小化，不同于在线性回归中是将均方误差最小化。
线性回归的应用[编辑]
趋势线[编辑]
一条趋势线代表着时间序列数据的长期走势。它告诉我们一组特定数据（如GDP、石油价格和股票价格）是否在一段时期内增长或下降。虽然我们可以用肉眼观察数据点在坐标系的位置大体画出趋势线，更恰当的方法是利用线性回归计算出趋势线的位置和斜率。
流行病学[编辑]
有关吸烟对死亡率和发病率影响的早期证据来自采用了回归分析的观察性研究。为了在分析观测数据时减少伪相关，除最感兴趣的变量之外,通常研究人员还会在他们的回归模型里包括一些额外变量。例如，假设我们有一个回归模型，在这个回归模型中吸烟行为是我们最感兴趣的独立变量，其相关变量是经数年观察得到的吸烟者寿命。研究人员可能将社会经济地位当成一个额外的独立变量，已确保任何经观察所得的吸烟对寿命的影响不是由于教育或收入差异引起的。然而，我们不可能把所有可能混淆结果的变量都加入到实证分析中。例如，某种不存在的基因可能会增加人死亡的几率，还会让人的吸烟量增加。因此，比起采用观察数据的回归分析得出的结论，随机对照试验常能产生更令人信服的因果关系证据。当可控实验不可行时，回归分析的衍生，如工具变量回归，可尝试用来估计观测数据的因果关系。
金融[编辑]
资本资产定价模型利用线性回归以及Beta系数的概念分析和计算投资的系统风险。这是从联系投资回报和所有风险性资产回报的模型Beta系数直接得出的。
经济学[编辑]
线性回归是经济学的主要实证工具。例如，它是用来预测消费支出，固定投资支出，存货投资，一国出口产品的购买，进口支出，要求持有流动性资产，劳动力需求、劳动力供给。
参考文献[编辑]
引用[编辑]

来源[编辑]

书籍


Cohen, J., Cohen P., West, S.G., & Aiken, L.S. Applied multiple regression/correlation analysis for the behavioral sciences. Hillsdale, NJ: Lawrence Erlbaum Associates. 2003. 
Draper, N.R. and Smith, H. Applied Regression Analysis. Wiley Series in Probability and Statistics. 1998. 
Robert S. Pindyck and Daniel L. Rubinfeld. Chapter One. Econometric Models and Economic Forecasts. 1998. 
Charles Darwin. The Variation of Animals and Plants under Domestication. (1868) (Chapter XIII describes what was known about reversion in Galton's time. Darwin uses the term "reversion".)


刊物文章


Galton, Francis. Regression Towards Mediocrity in Hereditary Stature (PDF). Journal of the Anthropological Institute. 1886, 15: 246–263 [2008-12-30]. 

延伸阅读[编辑]

Pedhazur, Elazar J. Multiple regression in behavioral research: Explanation and prediction 2nd. New York: Holt, Rinehart and Winston. 1982. ISBN 0-03-041760-0. 
Barlow, Jesse L. Chapter 9: Numerical aspects of Solving Linear Least Squares Problems. (编) Rao, C.R. Computational Statistics. Handbook of Statistics 9. North-Holland. 1993. ISBN 0-444-88096-8 
Björck, Åke. Numerical methods for least squares problems. Philadelphia: SIAM. 1996. ISBN 0-89871-360-9. 
Goodall, Colin R. Chapter 13: Computation using the QR decomposition. (编) Rao, C.R. Computational Statistics. Handbook of Statistics 9. North-Holland. 1993. ISBN 0-444-88096-8 
National Physical Laboratory. Chapter 1: Linear Equations and Matrices: Direct Methods. Modern Computing Methods. Notes on Applied Science 16 2nd. Her Majesty's Stationery Office. 1961 

参见[编辑]


方差分析
安斯库姆四重奏
横截面回归
曲线拟合
经验贝叶斯方法
逻辑斯蒂回归
M估计
非线性回归
非参数回归
多元自适应回归样条
Lack-of-fit sum of squares
截断回归模型
删失回归模型
简单线性回归
分段线性回归











查
论
编


试验设计






科学方法



科学实验
统计设计
控制
内部效度 & 外部效度
Experimental unit
双盲实验


Optimal design: Bayesian
Random assignment
Randomization
Restricted randomization
Replication versus subsampling
Sample size








实验
& 阻塞



Treatment
效应值
Contrast
Interaction
Confounding
正交
Blocking
Covariate
Nuisance variable








概率模型
& 推论统计学



线性回归
Ordinary least squares
Bayesian


Random effect
Mixed model
等级线性模型: 贝氏网络


ANOVA
Cochran's theorem
Manova (multivariate)
Ancova (covariance)


Compare means
Multiple comparison








试验设计:

完全随机设计



Factorial
Fractional factorial
Plackett-Burman
田口方法


反应曲面法
Polynomial & rational modeling
Box-Behnken
Central composite


Block
Generalized randomized block design (GRBD)
拉丁方阵
希腊拉丁方阵
Orthogonal array
拉丁超立方
Repeated measures design
Crossover study


随机对照试验
Sequential analysis
Sequential probability ratio test











Glossary
实验设计分类
概率与统计
Statistical outline
Statistical topics
















查
论
编


统计学






描述统计学





连续概率






集中趋势


平均数（平方 · 算术 · 几何 · 调和 · 算术-几何 · 几何-调和 · 希罗|平均数不等式） · 中位数 · 众数







离散程度


全距 · 标准差 · 变异系数 · 百分位数 · 四分差 · 四分位数 · 方差 · 标准分数 · 切比雪夫不等式







分布形态（英语：Shape of the distribution）


偏态 · 峰态










离散概率


次数（英语：Count data） · 列联表（英语：Contingency table）












推论统计学
和 假设检定





推论统计学


置信区间 · 区间估计（英语：Interval estimation） · 显著性差异 · 元分析 · 贝氏推论







实验设计


统计总量 · 抽样 · 重复（英语：Replication (statistics)） · 阻碍 · 特敏度 · 区集（英语：Blocking (statistics)）







样本量（英语：Sample size）


统计功效 · 效应值 · 标准误 · 虚无假设 · 对立假设（英语：Alternative hypothesis） · 第一型和第二型误差 · 统计检定力







常规估计


贝叶斯推论 · 区间估计（英语：Interval estimation） · 最大似然估计 · 最小距离估计（英语：Minimum distance estimation） · 矩量法 · 最大间距







特效检验


Z检验 · 学生t检验 · F检验 · 卡方检验 · Wald检验（英语：Wald test） · 曼-惠特尼检验（英语：Mann–Whitney U test） · 秩和检验







生存分析


生存函数 · 乘积极限估计量 · 对数秩和检定 · 失效率 · 危险比例模式









相关及
回归分析





相关性


混淆变项（英语：Confounding） · 皮尔森积差相关系数 · 等级相关（英语：Rank correlation） (史匹曼等级相关系数 · 肯德等级相关系数（英语：Kendall tau rank correlation coefficient）)







线性回归


线性模式（英语：Linear model） · 一般线性模式 · 广义线性模式 · 方差分析 · 协方差分析（英语：Analysis of covariance）







非线性回归（英语：Nonlinear regression）


非参数回归模型（英语：Nonparametric regression） · 半参数回归模型（英语：Semiparametric regression） · Logit模型









统计图形

饼图 · 条形图 · 双标图 · 箱形图 · 管制图 · 森林图（英语：Forest plot） · 直方圖 · QQ图 · 趋势图 · 散布图（英语：Scatter plot） · 茎叶图（英语：Stem-and-leaf display） · 雷达图（英语：Radar chart） · 示意地图









分类
主题
共享资源
 专题












分类：回归分析估计理论隐藏分类：自2013年10月包含过多行话或专业术语的条目粗劣翻译含有多个问题的条目有未列明来源语句的条目参考文献格式不符的条目