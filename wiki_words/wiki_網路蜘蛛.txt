網路爬蟲






本條目存在以下問題，請協助改善本條目或在討論頁針對議題發表看法。






本条目需要擴充。（2015年3月15日） 
请協助改善这篇條目，更進一步的信息可能會在討論頁或扩充请求中找到。请在擴充條目後將此模板移除。 







本条目需要精通或熟悉相关主题的编者参与及协助编辑。（2015年3月15日） 
請邀請適合的人士改善本条目。更多的細節與詳情請參见討論頁。 







本条目需要补充更多来源。（2015年3月15日） 
请协助添加多方面可靠来源以改善这篇条目，无法查证的内容可能會因為异议提出而移除。 







本条目可参照英語維基百科相應條目来扩充。 
若您熟悉来源语言和主题，请协助参考外语维基扩充条目。请勿直接提交机械翻译，也不要翻译不可靠、低品质内容。依版权协议，译文需在编辑摘要注明来源，或于讨论页顶部标记{{Translated page}}标签。 
















某爬虫的结构


网络爬虫（英语：web crawler），也叫網路蜘蛛（spider），是一种用来自动浏览万维网的网络机器人（英语：Internet bot）。其目的一般为编纂网络索引（英语：Web indexing）。
網路搜索引擎等站点通过爬蟲軟體更新自身的網站內容（英语：Web content）或其對其他網站的索引。網路爬蟲可以將自己所訪問的頁面保存下來，以便搜索引擎事後生成索引（英语：Index (search engine)）供用戶搜索。
爬蟲访问网站的过程会消耗目标系统资源。不少网络系统并不默许爬虫工作。因此在访问大量页面时，爬虫需要考虑到规划、负载，还需要讲“礼貌”。 不愿意被爬虫访问、被爬虫主人知晓的公开站点可以使用robots.txt文件之类的方法避免访问。这个文件可以要求机器人（英语：Software agent）只对网站的一部分进行索引，或完全不作处理。
互联网上的页面极多，即使是最大的爬虫系统也无法做出完整的索引。因此在公元2000年之前的万维网出现初期，搜索引擎经常找不到多少相关结果。现在的搜索引擎在这方面已经进步很多，能够即刻给出高质量结果。
爬虫还可以验证超連結和HTML代码，用于网络抓取（英语：Web scraping）（参见数据驱动编程（英语：Data-driven programming））。



目录


1 命名
2 概述
3 爬虫策略

3.1 选择策略

3.1.1 链接跟随限制
3.1.2 URL规范化
3.1.3 路径上移爬取
3.1.4 主题爬取


3.2 重新访问策略
3.3 平衡礼貌策略
3.4 并行策略


4 另見
5 参考文献



命名[编辑]
网络爬虫也可称作网络蜘蛛[1]、蚂蚁、自动索引程序（automatic indexer）[2] ，或（在FOAF（英语：FOAF (software)）软件中）称为网络疾走（web scutter）。[3]
概述[编辑]





本条目翻譯品質不佳。 
翻譯者可能不熟悉中文或原文語言，也可能使用了機器翻譯，請協助翻譯本條目或重新編寫。明顯拙劣的機器翻譯請改掛{{Delete|G13}}提交刪除。 


网络爬虫始于一张被称作种子的统一资源地址（URL）列表。当网络爬虫访问这些统一资源定位器时，它们会甄别出页面上所有的超链接，并将它们写入一张“待访列表”，即所谓爬行疆域（英语：crawl frontier）（crawl frontier）。此疆域上的诸URL将被按照一套策略循环访问。如果爬虫在他执行的过程中复制归档和保存网站上的信息，这些档案通常储存，使他们可以被查看。阅读和浏览他们的网站上实时更新的信息，并保存为网站的“快照”。大容量的体积意味着网络爬虫只能在给定时间内下载有限数量的网页，所以要优先考虑其下载。高变化率意味着网页可能已经被更新或者删除。一些被服务器端软件生成的URL（统一资源定位符）也使得网络爬虫很难避免检索到重复内容。
互联网资源卷帙浩繁，这意味着网络爬虫在一定时间内只能下载有限数量的网页，因此它需要优化它的下载方式。互联网资源瞬息万变，这也意味着网络爬虫下载的网页在使用前就已经被修改甚至是删除了。这是网络爬虫设计者们所面临的两个基本问题。
再者，服务器端软件所生成的统一资源地址数量庞大，以至于网络爬虫难以避免的采集到重复内容。根据超文本协议“显示请求”（HTTP GET）的参数的无尽组合所返回的页面中，只有很少一部分确实传回唯一的内容。例如：一个照片陈列室网站，可能通过几个参数，让用户选择相关照片：其一是通过四种方法对照片排序，其二是关于照片分辨率的的三种选择，其三是两种文件格式，另加一个用户可否提供内容的选择，这样对于同样的结果極可能会有48种不同的统一资源地址与其关联。这种数学组合给网络爬虫制造了麻烦，因为它们必须越过这些无关脚本变化的组合，寻找到不重复的内容。
爬虫策略[编辑]
爬虫的实现由以下策略组成：[4]

指定页面下载的选择策略
检测页面是否改变的重新访问策略
定义如何避免网站过度访问的约定性策略
如何部署分布式网络爬虫的并行策略

选择策略[编辑]
链接跟随限制[编辑]
爬虫可能只想搜索HTML页面而避免其他MIME 类型。为了只请求HTML资源，爬虫在抓取整个以GET方式请求的资源之前，通过创建HTTP的HEAD请求来决定网络资源的MIME类型。为了避免发出过多的请求，爬虫会检查URL和只请求那些以某些字符（如.html, .htm, .asp, .aspx, .php, .jsp, .jspx 或 / ）作为后缀的URL。这个策略可能会跳过很多HTML网络资源。
有些爬虫还能避免请求一些带有“?”的资源（动态生成）。为了避免掉入从网站下载无限量的URL的爬虫陷阱。不过假若网站重写URL以简化URL的目的，这个策略就变得不可靠了。
URL规范化[编辑]
主条目：URL规范化
爬虫通常使用某些URL规范化的方式以避免资源的重复爬取。URL规范化，指的是以某种一致的方式修改和标准化URL的过程。这个过程有各种各样的处理规则，包括统一转换为小写、移除“.”和“..”片段，以及在非空路径里插入斜杆。
路径上移爬取[编辑]
有些爬虫希望从指定的网站中尽可能地爬取资源。而路径上移爬虫就是为了能爬取每个URL里提示出的每个路径。[5] 例如，给定一个Http的种子URL: http://llama.org/hamster/monkey/page.html ，要爬取 /hamster/monkey/ ， /hamster/ 和 / 。Cothey发现路径能非常有效地爬取独立的资源，或以某种规律无法在站内链接爬取到的资源。
主题爬取[编辑]
主条目：主题爬虫
对于爬虫来说，一个页面的重要性也可以说是，给定查询条件一个页面相似性能起到的作用。网络爬虫要下载相似的网页被称为主题爬虫或局部爬虫。这个主题爬虫或局部爬虫的概念第一次被Filippo Menczer[6][7] 和 Soumen Chakrabarti 等人提出的。[8]
重新访问策略[编辑]
网站的属性之一就是经常动态变化，而爬取网站的一小部分往往需要花费几个星期或者几个月。等到网站爬虫完成它的爬取，很多事件也已经发生了，包括增加、更新和删除。 在搜索引擎的角度，因为没有检测这些变化，会导致存储了过期资源的代价。最常用的估价函数是新鲜度和过时性。 新鲜度：这是一个衡量抓取内容是不是准确的二元值。在时间t内，仓库中页面p的新鲜度是这样定义的：






F

p


(
t
)
=


{



1




i
f


 
p
 


 
i
s
 
e
q
u
a
l
 
t
o
 
t
h
e
 
l
o
c
a
l
 
c
o
p
y
 
a
t
 
t
i
m
e


 
t




0




o
t
h
e
r
w
i
s
e










{\displaystyle F_{p}(t)={\begin{cases}1&{\rm {if}}~p~{\rm {~is~equal~to~the~local~copy~at~time}}~t\\0&{\rm {otherwise}}\end{cases}}}



过时性:这是一个衡量本地已抓取的内容过时程度的指标。在时间t时，仓库中页面p的时效性的定义如下：






A

p


(
t
)
=


{



0




i
f


 
p
 


 
i
s
 
n
o
t
 
m
o
d
i
f
i
e
d
 
a
t
 
t
i
m
e


 
t




t
−


m
o
d
i
f
i
c
a
t
i
o
n
 
t
i
m
e
 
o
f


 
p




o
t
h
e
r
w
i
s
e










{\displaystyle A_{p}(t)={\begin{cases}0&{\rm {if}}~p~{\rm {~is~not~modified~at~time}}~t\\t-{\rm {modification~time~of}}~p&{\rm {otherwise}}\end{cases}}}



平衡礼貌策略[编辑]
爬虫相比于人，可以有更快的检索速度和更深的层次，所以，他们可能使一个站点瘫痪。不需要说一个单独的爬虫一秒钟要执行多条请求，下载大的文件。一个服务器也会很难响应多线程爬虫的请求。 就像Koster所注意的那样，爬虫的使用对很多工作都是很有用的，但是对一般的社区，也需要付出代价。使用爬虫的代价包括：[9]

网络资源：在很长一段时间，爬虫使用相当的带宽高度并行地工作。
服务器超载：尤其是对给定服务器的访问过高时。
质量糟糕的爬虫，可能导致服务器或者路由器瘫痪，或者会尝试下载自己无法处理的页面。
个人爬虫，如果过多的人使用，可能导致网络或者服务器阻塞。

对这些问题的局部解决方法是漫游器排除协议（Robots exclusion protocol），也被称为robots.txt议定书[10]，这份协议是让管理员指明网络服务器的不应该爬取的约定。这个标准没有包括重新访问一台服务器的间隔的建议，虽然设置访问间隔是避免服务器超载的最有效办法。最近的商业搜索引擎，如Google，Ask Jeeves，MSN和Yahoo可以在robots.txt中使用一个额外的 “Crawl-delay”参数来指明请求之间的延迟。
并行策略[编辑]
主条目：Distributed web crawling
一个并行爬虫是并行运行多个进程的爬虫。它的目标是最大化下载的速度，同时尽量减少并行的开销和下载重复的页面。为了避免下载一个页面两次，爬虫系统需要策略来处理爬虫运行时新发现的URL，因为同一个URL地址，可能被不同的爬虫进程抓到。
另見[编辑]

robots.txt：放在網頁伺服器上，告知網路蜘蛛哪些頁面內容可取得或不可取得。

参考文献[编辑]


^ Spetka, Scott. The TkWWW Robot: Beyond Browsing. NCSA. [21 November 2010]. （原始内容存档于3 September 2004）. 
^ Kobayashi, M. & Takeda, K. Information retrieval on the web. ACM Computing Surveys (ACM Press). 2000, 32 (2): 144–173. doi:10.1145/358923.358934. 
^ 参见FOAF项目wiki对“scutter”的描述
^ Castillo, Carlos. Effective Web Crawling (Ph.D. thesis). University of Chile. 2004 [2010-08-03]. 
^ Cothey, Viv. Web-crawling reliability. Journal of the American Society for Information Science and Technology. 2004, 55 (14): 1228–1238. doi:10.1002/asi.20078. 
^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery. In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann
^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments. In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press
^ Chakrabarti, S., van den Berg, M., and Dom, B. (1999). Focused crawling: a new approach to topic-specific web resource discovery. Computer Networks, 31(11–16):1623–1640.
^ E. G. Coffman Jr; Zhen Liu; Richard R. Weber. Optimal robot scheduling for Web search engines. Journal of Scheduling. 1998, 1 (1): 15–29. doi:10.1002/(SICI)1099-1425(199806)1:1<15::AID-JOS3>3.0.CO;2-K. 
^ Koster, M. (1996). A standard for robot exclusion.








规范控制



GND: 4796298-7












分类：搜索隐藏分类：自2015年3月扩充中的条目自2015年3月需要专业人士关注的页面自2015年3月需补充来源的条目拒绝当选首页新条目推荐栏目的条目需要從英語維基百科翻譯的條目含有多个问题的条目含有英語的條目粗劣翻译包含规范控制信息的维基百科条目